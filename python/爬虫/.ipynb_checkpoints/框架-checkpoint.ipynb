{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 简介\n",
    "- 爬虫框架\n",
    "- 异步下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 流程\n",
    "- 1. 创建一个scrapy项目\n",
    "    - `scrapy startproject mySpider`\n",
    "- 2. 生成一个爬虫\n",
    "    - `scrapy genspider itcast 'itcast.cn'`\n",
    "- 3. 提取数据\n",
    "    - `完善spider`\n",
    "- 4. 保存数据\n",
    "    - pipeline中保存数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 启动\n",
    "- scrapy crawl SPIDER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. spider demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItcastSpider(scrapy.Spider):\n",
    "    name = 'itcast'  # 爬虫名, 爬虫启动的时候使用\n",
    "    allowed_domains = ['itcast.cn']  # 允许爬取的范围\n",
    "    start_urls = ['http://www.itcast.cn/channel/teacher.shtml']  # 开始路径\n",
    "\n",
    "    def parse(self, response): # 数据提取方法， 接收下载中间件传过来的response\n",
    "        ret1 = response.xpath(\"//div[@class='tea_con']//h3/text()\").extract()\n",
    "        print(\"---------->:\", ret1)\n",
    "        \n",
    "        yield ret1[0]  # spider 的数据传到 pipeline\n",
    "        \n",
    "# 在选择器中提取字符串\n",
    "# 1. extract() 返回一个含有字符串数据的列表\n",
    "# 2. extract_first() 返回列表中的第一个字符串\n",
    "\n",
    "# 注意：\n",
    "# 1. spider 中的parse方法名不能修改\n",
    "# 2. 需要爬取的url地址必须要属于allow_domain下的连接\n",
    "# 3. response.xpath() 返回的是一个含有selector对象的列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. pipeline demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo\n",
    "class MyspiderPipeline(object):\n",
    "    \n",
    "    def open_spider(self, spider): //在爬虫开启的时候执行，仅执行一次\n",
    "        pass\n",
    "    \n",
    "    def close_spider(self, spider):  //在爬虫关闭的时候执行，仅执行一次\n",
    "        pass\n",
    "    \n",
    "    def process_item(self, item, spider): \n",
    "        item['hello'] = 'word'\n",
    "        return item\n",
    "    \n",
    "# 开启 pipeline， 在setting中设置开启\n",
    "ITEM_PIPELINES = {\n",
    "   'myspider.pipelines.MyspiderPipeline': 300,\n",
    "}\n",
    "\n",
    "# 注意：\n",
    "# 1. pipeline 权重越小，优先级越高\n",
    "# 2. process_item 方法名不能修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. logging 模块\n",
    "- 配置： 在 setting 中 :\n",
    "    - 设置日志级别：LOG_LEVEL = 'INFO'\n",
    "    - 指定日志文件：LOG_FILE = './log.log', 设置后终端不会显示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 设置日志输出样式（普通项目）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n",
    "                datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "                filename='myapp.log',\n",
    "                filemode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- logging.basicConfig函数各参数:\n",
    "    - filename: 指定日志文件名\n",
    "    - filemode: 和file函数意义相同，指定日志文件的打开模式，'w'或'a'\n",
    "    - format: 指定输出的格式和内容，format可以输出很多有用信息，如上例所示:\n",
    "         - %(levelno)s: 打印日志级别的数值\n",
    "         - %(levelname)s: 打印日志级别名称\n",
    "         - %(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0]\n",
    "         - %(filename)s: 打印当前执行程序名\n",
    "         - %(funcName)s: 打印日志的当前函数\n",
    "         - %(lineno)d: 打印日志的当前行号\n",
    "         - %(asctime)s: 打印日志的时间\n",
    "         - %(thread)d: 打印线程ID\n",
    "         - %(threadName)s: 打印线程名称\n",
    "         - %(process)d: 打印进程ID\n",
    "         - %(message)s: 打印日志信息\n",
    "    - datefmt: 指定时间格式，同time.strftime()\n",
    "    - level: 设置日志级别，默认为logging.WARNING\n",
    "    - stream: 指定将日志的输出流，可以指定输出到sys.stderr,sys.stdout或者文件，默认输出到sys.stderr，当stream和filename同时指定时，stream被忽略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 翻页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 翻页demo\n",
    "\n",
    "def parse(self, response):\n",
    "    tr_list = response.xpath(\"//table[@class='tablelist']//tr\")[1:-1]\n",
    "    for tr in tr_list:\n",
    "        item = {}\n",
    "        item['title'] = tr.xpath(\"./td[1]/a/text()\").extract_first()\n",
    "        item['position'] = tr.xpath(\"./td[2]/text()\").extract_first()\n",
    "        item['publish_date'] = tr.xpath(\"./td[5]/text()\").extract_first()\n",
    "        yield item\n",
    "    # 找到下一页的url地址\n",
    "    next_url = response.xpath(\"//a[@id='next']/@href\").extract_first()\n",
    "    if next_url != 'javascript:;':\n",
    "        next_url = 'https://hr.tencent.com/' + next_url\n",
    "        yield scrapy.Request(\n",
    "            next_url,\n",
    "            callback=self.parse\n",
    "        )\n",
    "\n",
    "# scrapy.Request常用参数为： \n",
    "#   callback： 指定传入的url交给那个解析函数去处理\n",
    "#   meta： 实现在不同的解析函数中传递数据， meta默认会携带部分信息， 比如：下载延迟、请求深度等\n",
    "#   dont_filter: 让scrapy的去重不会过滤当前url，scrapy默认有url去重功能，对需要重复请求的url有重要作用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. scrapy.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义Item\n",
    "class TencentItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    position = scrapy.Field()\n",
    "    publish_date = scrapy.Field()\n",
    "\n",
    "# 处理 Item\n",
    "class TencentPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        # 判断 item 类型\n",
    "        if isinstance(item, TencentItem):\n",
    "            print(item)\n",
    "            collection.insert(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 带详情页-处理\n",
    "class YgSpider(scrapy.Spider):\n",
    "    name = 'yg'\n",
    "    allowed_domains = ['sun0769.com']\n",
    "    start_urls = ['http://wz.sun0769.com/index.php/question/questionType?type=4']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # 分组\n",
    "        tr_list = response.xpath(\"//div[@id='morelist']//table[2]//table//tr\")\n",
    "        for tr in tr_list:\n",
    "            item = SunItem()\n",
    "            item['title'] = tr.xpath(\"./td[2]/a[@class='news14']/@title\").extract_first()\n",
    "            item['href'] = tr.xpath(\"./td[2]/a[@class='news14']/@href\").extract_first()\n",
    "            item['publish_date'] = tr.xpath(\"./td[last()]/text()\").extract_first()\n",
    "            yield scrapy.Request(\n",
    "                item['href'],\n",
    "                callback=self.parse_detail,\n",
    "                meta={\n",
    "                    'item': item\n",
    "                }\n",
    "            )\n",
    "        # 下一页\n",
    "        next_url = response.xpath(\"//div[@class='pagination']/a[text()='>']/@href\").extract_first()\n",
    "        if next_url is not None:\n",
    "            yield scrapy.Request(\n",
    "                next_url,\n",
    "                callback=self.parse\n",
    "            )\n",
    "\n",
    "    def parse_detail(self, response):\n",
    "        item = response.meta['item']\n",
    "\n",
    "        item['content'] = response.xpath(\"//div[@class='c1 text14_2']/div[@class='contentext']/text()\").extract()\n",
    "        item['content_img'] = response.xpath(\"//div[@class='c1 text14_2']//img/@src\").extract()\n",
    "        item['content_img'] = ['http://wz.sun0769.com'+i for i in item['content_img']]\n",
    "        yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. scrapy.shell\n",
    "- 使用方法：\n",
    "    - scrapy shell www.baidu.com\n",
    "- response.url : 当前响应的url地址\n",
    "- response.request.url : 当前响应对应的请求url地址\n",
    "- response.headers : 响应头\n",
    "- response.body : 响应体，默认byte类型\n",
    "- response.request.headers : 当前响应的请求头"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. scrapy setting 文件\n",
    "- 使用：\n",
    "    - 在 spider 中： self.settings.get('key')\n",
    "    - 在 pipeline 中： spider.settings.get('key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. scrapy crawlspider 文件\n",
    "- 创建爬虫： scrapy genspider -t crawl cf circ.gov.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.1 demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CfSpider(CrawlSpider):\n",
    "    name = 'cf'\n",
    "    allowed_domains = ['circ.gov.cn']\n",
    "    start_urls = ['http://bxjg.circ.gov.cn/web/site0/tab5240/module14430/page1.htm']\n",
    "\n",
    "    rules = (\n",
    "        # LinkExtractor 连接提出器， 提取url地址\n",
    "        # callback 提取出来的url地址的response会交给callback处理\n",
    "        # follow 当前url地址响应是否重新经过rules来提出url地址\n",
    "        Rule(LinkExtractor(allow=r'/web/site0/tab5240/info\\d+\\.htm'), callback='parse_item'),\n",
    "        Rule(LinkExtractor(allow=r'/web/site0/tab5240/module14430/page\\d+\\.htm'), follow=True),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        item = {}\n",
    "        item['title'] = re.findall(\"<!--TitleStart-->(.*?)<!--TitleEnd-->\", response.body.decode())[0]\n",
    "        item['publish_date'] = re.findall(\"发布时间：20\\d{2}-\\d{2}-\\d{2}\", response.body.decode())[0]\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. scrapy 模拟登录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo1\n",
    "class Gt2Spider(scrapy.Spider):\n",
    "    name = 'gt2'\n",
    "    allowed_domains = ['github.com']\n",
    "    start_urls = ['https://github.com/login']\n",
    "\n",
    "    def parse(self, response):\n",
    "        yield scrapy.FormRequest.from_response(\n",
    "            response,  # 自动的从response中寻找from表单\n",
    "            formdata={'login': 'layueyu', 'password': 'Bing199128'},\n",
    "            callback=self.after_login\n",
    "        )\n",
    "\n",
    "    def after_login(self, response):\n",
    "        with open('a.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(response.body.decode())\n",
    "        print(re.findall('layueyu', response.body.decode()))\n",
    "        \n",
    "# demo2\n",
    "class GtSpider(scrapy.Spider):\n",
    "    name = 'gt'\n",
    "    allowed_domains = ['github.com']\n",
    "    start_urls = ['https://github.com/login']\n",
    "\n",
    "    def parse(self, response):\n",
    "        authenticity_token = response.xpath(\"//input[@name='authenticity_token']/@value\").extract_first()\n",
    "        utf8 = response.xpath(\"//input[@name='utf8']/@value\").extract_first()\n",
    "        commit = response.xpath(\"//input[@name='commit']/@value\").extract_first()\n",
    "\n",
    "        post_data = dict(\n",
    "            login='layueyu',\n",
    "            password='Bing199128',\n",
    "            authenticity_token=authenticity_token,\n",
    "            commit=commit,\n",
    "            utf8=utf8\n",
    "        )\n",
    "\n",
    "        yield scrapy.FormRequest(\n",
    "            'https://github.com/session',\n",
    "            formdata=post_data,\n",
    "            callback=self.after_login\n",
    "        )\n",
    "\n",
    "    def after_login(self, response):\n",
    "        with open('a.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(response.body.decode())\n",
    "        print(re.findall('layueyu', response.body.decode()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. 下载中间件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例\n",
    "class LoginDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        # 添加自定以UA\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "        \n",
    "\n",
    "# 添加自定以UA\n",
    "class RandomUserAgent(object):\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        ua = random.choice(USER_AGENTS)\n",
    "        request.headers['User-Agent'] = ua\n",
    "\n",
    "# 添加代理， 在request的meta信息中添加proxy字段\n",
    "# 代理形式： 协议+ip地址+端口\n",
    "class ProxyMiddleware(object):\n",
    "    def process_request(self, request, spider):\n",
    "        request.meta['proxy'] = 'http://127.0.0.1:1234'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
